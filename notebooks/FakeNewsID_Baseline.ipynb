{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d5a9c69",
   "metadata": {},
   "source": [
    "\n",
    "# Fake News Detection (Bahasa Indonesia) — Baseline Notebook\n",
    "\n",
    "Notebook ini menyiapkan pipeline end-to-end untuk **Fake News Detection** pada teks Bahasa Indonesia:\n",
    "- Load data (`Data_latih.csv`, `Data_uji.csv`)\n",
    "- Preprocessing (lowercase, cleaning URL/punctuation, stopwords ID, stemming **Sastrawi**)\n",
    "- Ekstraksi fitur **TF–IDF**\n",
    "- Training baseline **Multinomial Naive Bayes**, **Logistic Regression**, **Linear SVM**\n",
    "- Evaluasi (Accuracy, Precision, Recall, F1, Confusion Matrix, ROC-AUC untuk model probabilistic)\n",
    "- Simpan artefak: vectorizer & model (folder `models/`)\n",
    "\n",
    "> Catatan: Pastikan file **Data_latih.csv** dan **Data_uji.csv** berada di folder yang sama dengan notebook ini, dan **memiliki kolom**: `text`, `label`.\n",
    "> Jika nama kolom berbeda, sesuaikan variabel `TEXT_COL` dan `LABEL_COL` di bawah ini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd3cd9",
   "metadata": {},
   "source": [
    "## 0. Persiapan Library (jalankan sekali saja di environment lokal/Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ce270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika perlu di Colab:\n",
    "# !pip install pandas numpy scikit-learn nltk Sastrawi matplotlib seaborn wordcloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aed797",
   "metadata": {},
   "source": [
    "## 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35797a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK stopwords downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\MSI\n",
      "[nltk_data]     ID\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
    "                             precision_recall_fscore_support, roc_auc_score, roc_curve)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "print('NLTK stopwords downloaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f33a34",
   "metadata": {},
   "source": [
    "## 2. Konfigurasi kolom & path file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051eee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: file data latih & uji ditemukan\n"
     ]
    }
   ],
   "source": [
    "# Konfigurasi kolom (ubah jika dataset Anda memakai nama berbeda)\n",
    "TEXT_COL = 'text'   # ganti jika perlu\n",
    "LABEL_COL = 'label' # ganti jika perlu\n",
    "\n",
    "TRAIN_PATH = 'Data_latih.csv'\n",
    "TEST_PATH  = 'Data_uji.csv'\n",
    "\n",
    "assert os.path.exists(TRAIN_PATH), f'File tidak ditemukan: {TRAIN_PATH}'\n",
    "assert os.path.exists(TEST_PATH),  f'File tidak ditemukan: {TEST_PATH}'\n",
    "print('OK: file data latih & uji ditemukan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ebcb2",
   "metadata": {},
   "source": [
    "## 3. Muat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e7093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom ['text'] tidak ditemukan di train. Otomatis mapping ke: text -> judul, label -> label\n",
      "Kolom ['text', 'label'] tidak ditemukan di test. Otomatis mapping ke: text -> judul, label -> nama file gambar\n",
      "Preview train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tanggal</th>\n",
       "      <th>text</th>\n",
       "      <th>narasi</th>\n",
       "      <th>nama file gambar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>17-Aug-20</td>\n",
       "      <td>Pemakaian Masker Menyebabkan Penyakit Legionna...</td>\n",
       "      <td>A caller to a radio talk show recently shared ...</td>\n",
       "      <td>71.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "      <td>17-Jul-20</td>\n",
       "      <td>Instruksi Gubernur Jateng tentang penilangan  ...</td>\n",
       "      <td>Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...</td>\n",
       "      <td>461.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>495</td>\n",
       "      <td>1</td>\n",
       "      <td>13-Jul-20</td>\n",
       "      <td>Foto Jim Rohn: Jokowi adalah presiden terbaik ...</td>\n",
       "      <td>Jokowi adalah presiden terbaik dlm sejarah ban...</td>\n",
       "      <td>495.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550</td>\n",
       "      <td>1</td>\n",
       "      <td>8-Jul-20</td>\n",
       "      <td>ini bukan politik, tapi kenyataan Pak Jokowi b...</td>\n",
       "      <td>Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...</td>\n",
       "      <td>550.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>681</td>\n",
       "      <td>1</td>\n",
       "      <td>24-Jun-20</td>\n",
       "      <td>Foto Kadrun kalo lihat foto ini panas dingin</td>\n",
       "      <td>Kadrun kalo lihat foto ini panas dingin . .</td>\n",
       "      <td>681.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  label    tanggal                                               text  \\\n",
       "0   71      1  17-Aug-20  Pemakaian Masker Menyebabkan Penyakit Legionna...   \n",
       "1  461      1  17-Jul-20  Instruksi Gubernur Jateng tentang penilangan  ...   \n",
       "2  495      1  13-Jul-20  Foto Jim Rohn: Jokowi adalah presiden terbaik ...   \n",
       "3  550      1   8-Jul-20  ini bukan politik, tapi kenyataan Pak Jokowi b...   \n",
       "4  681      1  24-Jun-20       Foto Kadrun kalo lihat foto ini panas dingin   \n",
       "\n",
       "                                              narasi nama file gambar  \n",
       "0  A caller to a radio talk show recently shared ...           71.jpg  \n",
       "1  Yth.Seluruh Anggota Grup Sesuai Instruksi Gube...          461.png  \n",
       "2  Jokowi adalah presiden terbaik dlm sejarah ban...          495.png  \n",
       "3  Maaf Mas2 dan Mbak2, ini bukan politik, tapi k...          550.png  \n",
       "4        Kadrun kalo lihat foto ini panas dingin . .          681.jpg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label train:\n",
      "label\n",
      "1    3465\n",
      "0     766\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Cek kolom otomatis jika TEXT_COL/LABEL_COL tidak ada\n",
    "def auto_detect_columns(df):\n",
    "    cols = [c.lower() for c in df.columns]\n",
    "    mapping = {}\n",
    "    # heuristics\n",
    "    text_candidates = [c for c in cols if 'text' in c or 'berita' in c or 'konten' in c or 'isi' in c or 'judul' in c]\n",
    "    label_candidates = [c for c in cols if 'label' in c or 'target' in c or 'tag' in c or 'kelas' in c or 'hoax' in c]\n",
    "    mapping['text'] = df.columns[cols.index(text_candidates[0])] if text_candidates else df.columns[0]\n",
    "    mapping['label'] = df.columns[cols.index(label_candidates[0])] if label_candidates else df.columns[-1]\n",
    "    return mapping\n",
    "\n",
    "for name, df in [('train', df_train), ('test', df_test)]:\n",
    "    missing = [c for c in [TEXT_COL, LABEL_COL] if c not in df.columns]\n",
    "    if missing:\n",
    "        md = auto_detect_columns(df)\n",
    "        print(f'Kolom {missing} tidak ditemukan di {name}. Otomatis mapping ke: text -> {md[\"text\"]}, label -> {md[\"label\"]}')\n",
    "        if name=='train':\n",
    "            df_train = df_train.rename(columns={md['text']:TEXT_COL, md['label']:LABEL_COL})\n",
    "        else:\n",
    "            df_test = df_test.rename(columns={md['text']:TEXT_COL, md['label']:LABEL_COL})\n",
    "\n",
    "print('Preview train:')\n",
    "display(df_train.head())\n",
    "print('Distribusi label train:')\n",
    "print(df_train[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065f75d",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Bahasa Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b553462e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pemakaian Masker Menyebabkan Penyakit Legionna...</td>\n",
       "      <td>pakai masker sebab sakit legionnaires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Instruksi Gubernur Jateng tentang penilangan  ...</td>\n",
       "      <td>instruksi gubernur jateng tilang masker muka 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Foto Jim Rohn: Jokowi adalah presiden terbaik ...</td>\n",
       "      <td>foto jim rohn jokowi presiden baik dlm sejarah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ini bukan politik, tapi kenyataan Pak Jokowi b...</td>\n",
       "      <td>politik nyata jokowi hasil pulang 000 triliun ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Foto Kadrun kalo lihat foto ini panas dingin</td>\n",
       "      <td>foto kadrun kalo lihat foto panas dingin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Pemakaian Masker Menyebabkan Penyakit Legionna...   \n",
       "1  Instruksi Gubernur Jateng tentang penilangan  ...   \n",
       "2  Foto Jim Rohn: Jokowi adalah presiden terbaik ...   \n",
       "3  ini bukan politik, tapi kenyataan Pak Jokowi b...   \n",
       "4       Foto Kadrun kalo lihat foto ini panas dingin   \n",
       "\n",
       "                                          clean_text  \n",
       "0              pakai masker sebab sakit legionnaires  \n",
       "1  instruksi gubernur jateng tilang masker muka 1...  \n",
       "2  foto jim rohn jokowi presiden baik dlm sejarah...  \n",
       "3  politik nyata jokowi hasil pulang 000 triliun ...  \n",
       "4           foto kadrun kalo lihat foto panas dingin  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('indonesian'))\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[\\w_]+\", \"\", text)      # mention\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)         # hashtag\n",
    "    text = re.sub(r\"[^a-z0-9 ]+\", \" \", text)  # keep alnum+space\n",
    "    tokens = [w for w in text.split() if w not in stop_words and len(w) > 2]\n",
    "    text = \" \".join(tokens)\n",
    "    text = stemmer.stem(text)\n",
    "    return text\n",
    "\n",
    "df_train['clean_text'] = df_train[TEXT_COL].astype(str).apply(clean_text)\n",
    "df_test['clean_text']  = df_test[TEXT_COL].astype(str).apply(clean_text)\n",
    "\n",
    "display(df_train[[TEXT_COL,'clean_text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e897058",
   "metadata": {},
   "source": [
    "## 5. Ekstraksi Fitur TF–IDF & Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f4553f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m pred = clf.predict(Xte)\n\u001b[32m     21\u001b[39m acc = accuracy_score(y_test, pred)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m p, r, f1, _ = \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweighted\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m results[name] = {\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: acc, \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m: p, \u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m: r, \u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m: f1}\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | P=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m R=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI ID\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI ID\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1830\u001b[39m, in \u001b[36mprecision_recall_fscore_support\u001b[39m\u001b[34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[39m\n\u001b[32m   1661\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[32m   1662\u001b[39m \n\u001b[32m   1663\u001b[39m \u001b[33;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1827\u001b[39m \u001b[33;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[32m   1828\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1829\u001b[39m _check_zero_division(zero_division)\n\u001b[32m-> \u001b[39m\u001b[32m1830\u001b[39m labels = \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[32m   1833\u001b[39m samplewise = average == \u001b[33m\"\u001b[39m\u001b[33msamples\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI ID\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1599\u001b[39m, in \u001b[36m_check_set_wise_labels\u001b[39m\u001b[34m(y_true, y_pred, average, labels, pos_label)\u001b[39m\n\u001b[32m   1596\u001b[39m y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n\u001b[32m   1597\u001b[39m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1599\u001b[39m present_labels = _tolist(\u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m average == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MSI ID\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:117\u001b[39m, in \u001b[36munique_labels\u001b[39m\u001b[34m(*ys)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMix of label input types (string and number)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[31mValueError\u001b[39m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "X_train_text = df_train['clean_text'].values\n",
    "y_train = df_train[LABEL_COL].values\n",
    "\n",
    "X_test_text = df_test['clean_text'].values\n",
    "y_test = df_test[LABEL_COL].values\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "Xtr = tfidf.fit_transform(X_train_text)\n",
    "Xte = tfidf.transform(X_test_text)\n",
    "\n",
    "models = {\n",
    "    'NaiveBayes': MultinomialNB(),\n",
    "    'LogReg': LogisticRegression(max_iter=2000, n_jobs=None),\n",
    "    'LinearSVM': LinearSVC()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in models.items():\n",
    "    clf.fit(Xtr, y_train)\n",
    "    pred = clf.predict(Xte)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_test, pred, average='weighted', zero_division=0)\n",
    "    results[name] = {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
    "    print(f'[{name}] acc={acc:.4f} | P={p:.4f} R={r:.4f} F1={f1:.4f}\\n')\n",
    "\n",
    "best_model_name = max(results, key=lambda k: results[k]['f1'])\n",
    "best_model = models[best_model_name]\n",
    "print('Best model:', best_model_name, results[best_model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dccdefd",
   "metadata": {},
   "source": [
    "## 6. Evaluasi Detail & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "pred_best = best_model.predict(Xte)\n",
    "print(classification_report(y_test, pred_best))\n",
    "cm = confusion_matrix(y_test, pred_best)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(xticks_rotation=45)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f620c0",
   "metadata": {},
   "source": [
    "## 7. Wordcloud (opsional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fdd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(texts, title):\n",
    "    wc = WordCloud(width=900, height=500, background_color='white').generate(\" \".join(texts))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "if LABEL_COL in df_train.columns:\n",
    "    for lbl in df_train[LABEL_COL].unique():\n",
    "        plot_wordcloud(df_train[df_train[LABEL_COL]==lbl]['clean_text'].tolist(), f'Wordcloud - {lbl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bac0c1",
   "metadata": {},
   "source": [
    "## 8. Simpan Artefak (Model & Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(tfidf, f'models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(best_model, f'models/{best_model_name}_model.pkl')\n",
    "print('Model & vectorizer tersimpan di folder models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39518560",
   "metadata": {},
   "source": [
    "## 9. Fungsi Prediksi untuk 1 Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text: str):\n",
    "    ct = clean_text(text)\n",
    "    X = tfidf.transform([ct])\n",
    "    return best_model.predict(X)[0]\n",
    "\n",
    "print(predict_text(\"Breaking: vaksin menyebabkan chip 5G di tubuh manusia, ini hoax?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
